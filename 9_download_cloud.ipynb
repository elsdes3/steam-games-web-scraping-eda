{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d6fa5f6-4e43-4bb6-b67d-710fdff87524",
   "metadata": {},
   "source": [
    "# Download data from Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97c3a16-8656-4533-8f0a-8db330a3f556",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc70a61-6535-4898-bb79-372e50495a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "from glob import glob\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from prefect import Flow, context as prefect_context, task, unmapped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e01401-3805-4895-877c-456935837584",
   "metadata": {},
   "source": [
    "<a href=\"table-of-contents\"></a>\n",
    "\n",
    "## [Table of Contents](#table-of-contents)\n",
    "0. [About](#about)\n",
    "1. [User Inputs](#user-inputs)\n",
    "2. [Download combined archive from Blob Storage and Extract raw data](#download-combined-archive-from-blob-storage-and-extract-raw-data)\n",
    "   - 2.1. [Retrieve Selenium-based Listings and Search Results from Cloud Storage](#retrieve-listings-and-search-results-from-cloud-storage)\n",
    "   - 2.2. [Retrieve Requests-based Listings and Search Results from Cloud Storage](#retrieve-requests-based-listings-and-search-results-from-cloud-storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f9c94a-3854-4079-b737-c987b4b4c5ab",
   "metadata": {},
   "source": [
    "<a id=\"about\"></a>\n",
    "\n",
    "## 0. [About](#about)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61463d56-123e-4cf8-b08f-0da804737c9b",
   "metadata": {},
   "source": [
    "Download all scraped listings and search results from [Azure blob storage](https://azure.microsoft.com/en-us/services/storage/blobs/).\n",
    "\n",
    "**Requirements**\n",
    "\n",
    "The following four environment variables should be exported before running this notebook\n",
    "- `BLOB_NAME_PREFIX`\n",
    "  - for each combined archive to be uploaded (listings and search results), a unique number will be appended to this value\n",
    "- `AZURE_STORAGE_ACCOUNT`\n",
    "- `AZURE_STORAGE_KEY`\n",
    "- `ENDPOINT_SUFFIX`\n",
    "\n",
    "**Notes**\n",
    "\n",
    "1. The workflow used here is executed using a workflow management tool. This was optional, but has been used here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54863608-d30e-44b0-a383-e3d51c7586bd",
   "metadata": {},
   "source": [
    "<a id=\"user-inputs\"></a>\n",
    "\n",
    "## 1. [User Inputs](#user-inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb29b84-1c21-4000-984b-c04d4662240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJ_ROOT_DIR = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53738eba-e864-4a4d-a613-47828f283eb9",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Suffixes (of filenames) to upload combined archives of listings and\n",
    "# search results to Azure blob storage\n",
    "blob_name_suffixes = {\n",
    "    \"listings\": 80,\n",
    "    \"search_results\": 81,\n",
    "    \"listings_requests\": 82,\n",
    "    \"search_results_requests\": 83,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda66fe4-a46e-4233-89fc-1842607cc31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(PROJ_ROOT_DIR, \"data\")\n",
    "raw_data_dir = os.path.join(data_dir, \"raw\")\n",
    "requests_files_dir = os.path.join(raw_data_dir, \"requests\")\n",
    "selenium_files_dir = os.path.join(raw_data_dir, \"selenium\")\n",
    "\n",
    "conn_str = (\n",
    "    \"DefaultEndpointsProtocol=https;\"\n",
    "    f\"AccountName={os.getenv('AZURE_STORAGE_ACCOUNT')};\"\n",
    "    f\"AccountKey={os.getenv('AZURE_STORAGE_KEY')};\"\n",
    "    f\"EndpointSuffix={os.getenv('ENDPOINT_SUFFIX')}\"\n",
    ")\n",
    "blob_name_prefix = os.getenv(\"BLOB_NAME_PREFIX\")\n",
    "\n",
    "# (For downloading previously scraped data from Azure cloud storage)\n",
    "# Create paths to data/raw/selenium and/or data/raw/requests\n",
    "if not os.path.exists(selenium_files_dir):\n",
    "    os.mkdir(selenium_files_dir)\n",
    "if not os.path.exists(requests_files_dir):\n",
    "    os.mkdir(requests_files_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba2c61d-c88f-4602-94fd-616dbbfa2afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@task\n",
    "def download_az_file_blobs(blob_names_dict, conn_str, az_container_name=\"myconedesx7\"):\n",
    "    logger = prefect_context.get(\"logger\")\n",
    "    # print(blob_names_dict)\n",
    "    downloaded_blobs = []\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(conn_str=conn_str)\n",
    "    for az_blob_name, local_file_path in blob_names_dict.items():\n",
    "        blob_client = blob_service_client.get_blob_client(\n",
    "            container=az_container_name, blob=az_blob_name\n",
    "        )\n",
    "        # print(blob_client, local_file_path)\n",
    "        local_filename = os.path.basename(local_file_path)\n",
    "        if not os.path.exists(local_file_path):\n",
    "            with open(local_file_path, \"wb\") as download_file:\n",
    "                download_stream = blob_client.download_blob()\n",
    "                download_file.write(download_stream.readall())\n",
    "            downloaded_blobs.append(local_file_path)\n",
    "            logger.info(\n",
    "                f\"Blob {az_blob_name} not found at {local_filename}. Downloaded to {local_filename}.\"\n",
    "            )\n",
    "        else:\n",
    "            logger.info(\n",
    "                f\"Blob {az_blob_name} found at {local_filename}. Did not download to {local_filename}.\"\n",
    "            )\n",
    "    return downloaded_blobs\n",
    "\n",
    "\n",
    "@task\n",
    "def unarchive(file_name, data_dir, search_str, flatten_filepaths=True):\n",
    "    logger = prefect_context.get(\"logger\")\n",
    "    if file_name:\n",
    "        if flatten_filepaths:\n",
    "            file_name = file_name[0]\n",
    "            # print(file_name)\n",
    "        with ZipFile(file_name) as zip_ref:\n",
    "            zip_ref.extractall(data_dir)\n",
    "        logger.info(\n",
    "            f\"Unarchived contents of {os.path.basename(file_name)} to \"\n",
    "            f\"{os.path.split(data_dir)[-1]}\"\n",
    "        )\n",
    "    else:\n",
    "        logger.info(f\"Got empty archive name. Did not unarchive\")\n",
    "    return glob(os.path.join(data_dir, search_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3762c7dd-96e0-43dd-a339-93e60a590835",
   "metadata": {},
   "source": [
    "<a id=\"download-combined-archive-from-blob-storage-and-extract-raw-data\"></a>\n",
    "\n",
    "## 2. [Download combined archive from Blob Storage and Extract raw data](#download-combined-archive-from-blob-storage-and-extract-raw-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd16bf41-d1d5-463f-a4ab-8bbaf321ea53",
   "metadata": {},
   "source": [
    "<a id=\"retrieve-selenium-based-listings-and-search-results-from-cloud-storage\"></a>\n",
    "\n",
    "### 2.1. [Retrieve Selenium-based Listings and Search Results from Cloud Storage](#retrieve-listings-and-search-results-from-cloud-storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111e10a0-7322-4459-9b2e-61d3ce3d5331",
   "metadata": {},
   "source": [
    "Change into the sub-directory where listings and search results scraped with `selenium` will be downloaded from the cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928457f8-8ad0-420a-8928-fc06d9701927",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(selenium_files_dir)\n",
    "print(f\"Current working directory is {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22835ba0-9e69-4645-98fb-705aea749572",
   "metadata": {},
   "source": [
    "Download the combined archives for batched listings and search results from Azure blob storage and extract the contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78e7cde-fadd-471b-843e-08847415c17a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "with Flow(\"Download and Unarchive listings and search results scraped with selenium\") as flow:\n",
    "    for file_type in [\"listings\", \"search_results\"]:\n",
    "        combo_archive = download_az_file_blobs(\n",
    "            {f\"{blob_name_prefix}{blob_name_suffixes[file_type]}\": os.path.join(selenium_files_dir, f\"combo_batched_{file_type}.zip\")},\n",
    "            conn_str,\n",
    "        )\n",
    "        unarchived_archives = unarchive(\n",
    "            combo_archive, selenium_files_dir, f\"batched_{file_type}_*.zip\", True\n",
    "        )\n",
    "        unarchived_files = unarchive.map(\n",
    "            unarchived_archives,\n",
    "            unmapped(selenium_files_dir),\n",
    "            unmapped(f\"{file_type}_*.parquet.gzip\"),\n",
    "            unmapped(False),\n",
    "        )\n",
    "\n",
    "state_query = flow.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361136a3-119b-45ca-879e-d82bf0c0cf00",
   "metadata": {},
   "source": [
    "<a id=\"retrieve-requests-based-listings-and-search-results-from-cloud-storage\"></a>\n",
    "\n",
    "### 2.2. [Retrieve Requests-based Listings and Search Results from Cloud Storage](#retrieve-requests-based-listings-and-search-results-from-cloud-storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24406f37-7477-42c9-a240-3b214a80477f",
   "metadata": {},
   "source": [
    "Change into the sub-directory where listings and search results scraped with `requests` will be downloaded from the cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2931f42a-74bb-46c4-aad6-fa76bfe0bfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(requests_files_dir)\n",
    "print(f\"Current working directory is {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475b6e65-a714-4122-933f-0729ec3fc20b",
   "metadata": {},
   "source": [
    "Download the combined archives for batched listings and search results from Azure blob storage and extract the contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b9aa64-4526-45a0-a1c6-f89743a709f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "with Flow(\"Download and Unarchive listings and search results scraped with requests\") as flow:\n",
    "    for file_type in [\"listings\", \"search_results\"]:\n",
    "        combo_archive = download_az_file_blobs(\n",
    "            {f\"{blob_name_prefix}{blob_name_suffixes[file_type+'_requests']}\": os.path.join(requests_files_dir, f\"combo_batched_{file_type}.zip\")},\n",
    "            conn_str,\n",
    "        )\n",
    "        unarchived_archives = unarchive(\n",
    "            combo_archive, requests_files_dir, f\"batched_{file_type}_*.zip\", True\n",
    "        )\n",
    "        unarchived_files = unarchive.map(\n",
    "            unarchived_archives,\n",
    "            unmapped(requests_files_dir),\n",
    "            unmapped(f\"{file_type}_*.parquet.gzip\"),\n",
    "            unmapped(False),\n",
    "        )\n",
    "\n",
    "state_query = flow.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96b08ce-abce-4910-bf3b-65463c7b79ed",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cd6237-6148-4825-ad67-3010b204eefc",
   "metadata": {},
   "source": [
    "<span style=\"float:left\">\n",
    "    <a href=\"./8_upload_cloud.ipynb\"><< 8 - Upload all scraped data (with requests and selenium) to cloud storage</a>\n",
    "</span>\n",
    "\n",
    "<span style=\"float:right\">\n",
    "    <a href=\"./10_selenium_to_db.ipynb\">10 - Clean and merge selenium search results and listings and append to MySQL database>></a>\n",
    "</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
